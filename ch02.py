# -*- coding: utf-8 -*-
"""ch02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q-xrpcwerJjhRnjDXWZF5PcE_B4YcTBo

##2-1. API 사용하기
"""

d = {"name": "혼자 공부하는 데이터 분석"}
print(d['name'])

import json

d_str = json.dumps(d, ensure_ascii=False)
print(d_str)

print(type(d_str))

d2 = json.loads(d_str)
print(d2['name'])

print(type(d2))

d3 = json.loads('{"name": "혼자 공부하는 데이터 분석", "author": "박해선", "year": 2022}')
print(d3['name'])
print(d3['author'])
print(d3['year'])

d3 = json.loads('{"name": "혼자 공부하는 데이터 분석", "author": ["박해선", "홍길동"], "year": 2022}')
print(d3['author'][1])

d4_str = """
[
    {"name": "혼자 공부하는 데이터 분석", "author": "박해선", "year": 2022},
    {"name": "혼자 공부하는 머신러닝+딥러닝", "author": "박해선", "year": 2020}
]
"""

d4 = json.loads(d4_str)
print(d4[0]['name'])

import pandas as pd
pd.read_json(d4_str)

pd.DataFrame(d4)

x_str = """
<book>
    <name>혼자 공부하는 데이터 분석</name>
    <author>박해선</author>
    <year>2022</year>
</book>
"""

import xml.etree.ElementTree as et
book = et.fromstring(x_str)

print(type(book))

print(book.tag)

book_childs = list(book)
print(book_childs)

name, author, year = book_childs
print(name.text)
print(author.text)
print(year.text)

name = book.findtext('name')
author = book.findtext('author')
year = book.findtext('year')

print(name)
print(author)
print(year)

x2_str = """
<books>
    <book>
        <name>혼자 공부하는 데이터 분석</name>
        <author>박해선</author>
        <year>2022</year>
    </book>
    <book>
        <name>혼자 공부하는 머신러닝+딥러닝</name>
        <author>박해선</author>
        <year>2020</year>
    </book>
</books>
"""

books = et.fromstring(x2_str)
print(books.tag)

for book in books.findall('book'):
    name = book.findtext('name')
    author = book.findtext('author')
    year = book.findtext('year')
    print(name)
    print(author)
    print(year)
    print()

"""### 도서관 정보나루 API
###### 승인완료 후 사용 가능 -> 승인 대기중
"""

# https://data4library.kr/api/loanItemSrch?authKey=521741ccfaf06ae6bca0fdbeaacc6324f3c2457dd0a04c3dc43427a2abe1f41f&startDt=2021-04-01&endDt=2021-04-30&age=20

"""파이썬으로 API 호출하기: requests 패키지"""

import requests

url = "https://data4library.kr/api/loanItemSrch?format=json&startDt=2021-04-01&endDt=2021-04-30&age=20&authKey=521741ccfaf06ae6bca0fdbeaacc6324f3c2457dd0a04c3dc43427a2abe1f41f"

r = requests.get(url, verify=False)

data = r.json()
print(data)

data

books = []
for d in data['response']['docs']:
    books.append(d['doc'])

books

books_df = pd.DataFrame(books)
books_df

books_df.to_json('20s_best_book.json')

"""## 2-2. 웹 스크래핑 사용하기"""

import gdown
gdown.download('https://bit.ly/3q9SZix', '20s_best_book.json', quiet=False)

import pandas as pd

books_df = pd.read_json('20s_best_book.json')
books_df.head()

books = books_df[['no', 'ranking', 'bookname', 'authors', 'publisher', 'publication_year', 'isbn13']]
books.head()

books_df.loc[[0, 1], ['bookname', 'authors']]

books_df.loc[0:1, 'bookname':'authors']

books = books_df.loc[:, 'no':'isbn13']
books.head()

import requests

isbn = 9791190090018
url = 'http://www.yes24.com/Product/Search?domain=Book&query={}'
r = requests.get(url.format(isbn))

print(r.text)

from bs4 import BeautifulSoup

soup = BeautifulSoup(r.text, 'html.parser')

prd_link = soup.find('a', attrs={'class':'gd_name'})
prd_link

print(prd_link['href'])

url = 'http://www.yes24.com' + prd_link['href']
r = requests.get(url)

print(r.text)

soup = BeautifulSoup(r.text, 'html.parser')
prd_detail = soup.find('div', attrs={'id':'infoset_specific'})
prd_detail

prd_tr_list = prd_detail.find_all('tr')
print(prd_tr_list)

for tr in prd_tr_list:
    if tr.find('th').get_text() == '쪽수, 무게, 크기':
        page_td = tr.find('td').get_text()
        break

print(page_td)

print(page_td.split()[0])

def get_page_cnt(isbn):
    # Yes24 도서 검색 페이지 URL
    url = 'http://www.yes24.com/Product/Search?domain=BOOK&query={}'
    
    # URL에 ISBN을 넣어 HTML 가져오기
    r = requests.get(url.format(isbn))
    soup = BeautifulSoup(r.text, 'html.parser')

    # 검색 결과에서 해당 도서 선택
    prd_info = soup.find('a', attrs={'class':'gd_name'})

    # 도서 상세 페이지 가져오기
    url = 'http://www.yes24.com' + prd_info['href']
    r = requests.get(url)
    soup = BeautifulSoup(r.text, 'html.parser')

    # 품목정보가 담긴 div 태그 선택
    prd_detail = soup.find('div', attrs={'id':'infoset_specific'})

    # table에 있는 tr 태그 가져오기
    prd_tr_list = prd_detail.find_all('tr')

    # 쪽수가 들어있는 <th>를 찾아 <td>에 담긴 값 반환
    for tr in prd_tr_list:
        if tr.find('th').get_text() == '쪽수, 무게, 크기':
            return tr.find('td').get_text().split()[0] # 쪽수만 반환

    return ''

get_page_cnt(9791190090018)

top10_books = books.head(10)

def get_page_cnt2(row):
    isbn = row['isbn13']
    return get_page_cnt(isbn)

page_count = top10_books.apply(get_page_cnt2, axis=1)
print(page_count)

page_count.name = 'page_count'
print(page_count)

top10_with_page_count = pd.merge(top10_books, page_count, left_index=True, right_index=True)
top10_with_page_count

